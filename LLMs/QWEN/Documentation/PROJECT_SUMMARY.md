# QWEN Fine-tuning Project Summary

## âœ… Project Complete!

I've successfully created a complete fine-tuning pipeline for QWEN LLMs to perform domain-specific English-Arabic translation.

---

## ğŸ“ What Was Created

### Directory Structure
```
Transformers-2/LLMs/QWEN/
â”œâ”€â”€ prepare_data.py          # Data preparation script
â”œâ”€â”€ train_qwen.py           # Single domain training
â”œâ”€â”€ train_all.py            # Batch training for all domains
â”œâ”€â”€ evaluate.py             # Model evaluation and inference
â”œâ”€â”€ app.py                  # Streamlit web UI
â”œâ”€â”€ quick_start.ipynb       # Jupyter notebook for testing
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ config.ini              # Configuration file
â”œâ”€â”€ setup.sh               # Bash setup script
â”œâ”€â”€ setup.fish             # Fish shell setup script
â”œâ”€â”€ README.md              # Complete documentation
â”œâ”€â”€ QUICKSTART.md          # Quick start guide
â””â”€â”€ .gitignore             # Git ignore file
```

---

## ğŸ¯ Key Features Implemented

### 1. **Domain-Specific Training**
- âœ… Technology domain
- âœ… Economic domain  
- âœ… Education/Research domain
- Each domain trained independently for optimal performance

### 2. **Efficient Fine-tuning**
- âœ… QLoRA (4-bit quantization) for memory efficiency
- âœ… LoRA adapters for parameter-efficient training
- âœ… Gradient checkpointing for reduced memory usage
- âœ… Works on consumer GPUs (16GB+ VRAM)

### 3. **Data Pipeline**
- âœ… Automatic loading of CSV parallel corpora
- âœ… Train/validation/test splits (80/10/10)
- âœ… QWEN chat template formatting
- âœ… Statistics and data quality checks

### 4. **Evaluation System**
- âœ… BLEU score calculation
- âœ… chrF score (character-level, better for Arabic)
- âœ… Example translation outputs
- âœ… Batch translation support

### 5. **Streamlit Web UI**
- âœ… Interactive translation interface
- âœ… Domain comparison tool
- âœ… Evaluation results visualization
- âœ… Translation history tracking
- âœ… Adjustable generation parameters

---

## ğŸš€ How to Use

### Quick Start (3 Steps)

1. **Install dependencies:**
   ```bash
   cd Transformers-2/LLMs/QWEN
   pip install -r requirements.txt
   ```

2. **Prepare data:**
   ```bash
   python prepare_data.py
   ```

3. **Train a model:**
   ```bash
   python train_qwen.py --domain technology
   ```

4. **Launch UI:**
   ```bash
   streamlit run app.py
   ```

### Or Use Automated Setup

```bash
./setup.fish  # For fish shell
# or
bash setup.sh  # For bash
```

---

## ğŸ“Š What Each Script Does

### `prepare_data.py`
- Loads CSV files from `../../Data/english-arabic/`
- Processes: `technology.csv`, `economic v1.csv`, `Education_Research.csv`
- Creates JSONL files with QWEN chat format
- Generates train/val/test splits
- Outputs to `data/prepared/`

**Run it:** `python prepare_data.py`

### `train_qwen.py`
- Fine-tunes QWEN for a specific domain
- Uses QLoRA for memory efficiency
- Saves model to `models/{domain}_model/`
- Supports custom hyperparameters

**Example:**
```bash
python train_qwen.py \
    --domain technology \
    --num_train_epochs 3 \
    --per_device_train_batch_size 4
```

### `train_all.py`
- Trains all three domains sequentially
- Automated pipeline for full training
- Progress tracking and error handling

**Run it:** `python train_all.py`

### `evaluate.py`
- Evaluates model on test set
- Calculates BLEU and chrF scores
- Generates example translations
- Supports interactive translation mode

**Examples:**
```bash
# Full evaluation
python evaluate.py --model_path models/technology_model --domain technology

# Quick translation
python evaluate.py --model_path models/technology_model --domain technology \
    --translate_text "Your English text here"
```

### `app.py`
- Streamlit web interface
- Four main features:
  1. **Translate Tab**: Single domain translation
  2. **Compare Domains**: Test same text across all models
  3. **Evaluation**: View performance metrics and charts
  4. **History**: Track translation history

**Launch it:** `streamlit run app.py`

---

## ğŸ’» System Requirements

### Minimum
- Python 3.8+
- 16GB GPU VRAM (NVIDIA with CUDA)
- 50GB disk space
- 16GB RAM

### Recommended
- Python 3.10+
- 24GB GPU VRAM (RTX 3090/4090, A5000)
- 100GB disk space
- 32GB RAM

### Training Time Estimates
With RTX 3090 (24GB):
- Technology: ~8 hours
- Economic: ~12 hours (larger dataset)
- Education: ~10 hours

**Total: ~30 hours for all domains**

---

## ğŸ“ Model Configuration

### Base Model
- **Default**: `Qwen/Qwen2-1.5B-Instruct` (fast, good quality)
- **Alternative**: `Qwen/Qwen2-7B-Instruct` (slower, better quality)

### LoRA Settings
- **Rank**: 16 (balance between quality and speed)
- **Alpha**: 32
- **Dropout**: 0.05
- **Target Modules**: All attention and FFN layers

### Training Hyperparameters
- **Learning Rate**: 2e-4
- **Batch Size**: 4 Ã— 4 (effective 16)
- **Epochs**: 3 (adjustable)
- **Optimizer**: Paged AdamW 32-bit
- **Precision**: BFloat16

---

## ğŸ“ˆ Expected Results

After 3 epochs of training:

| Domain | BLEU Score | chrF Score |
|--------|------------|------------|
| Technology | 35-45 | 60-70 |
| Economic | 30-40 | 55-65 |
| Education | 32-42 | 58-68 |

*Actual scores depend on data quality and training duration*

---

## ğŸ› ï¸ Customization Options

### Change Base Model
Edit in training command:
```bash
python train_qwen.py --model_name Qwen/Qwen2-7B-Instruct --domain technology
```

### Adjust Training Duration
```bash
python train_qwen.py --domain technology --num_train_epochs 5
```

### Reduce Memory Usage
```bash
python train_qwen.py --domain technology \
    --per_device_train_batch_size 2 \
    --max_seq_length 256
```

### Change LoRA Parameters
```bash
python train_qwen.py --domain technology \
    --lora_r 32 \
    --lora_alpha 64
```

---

## ğŸ“š Documentation Files

1. **README.md** - Complete documentation with all details
2. **QUICKSTART.md** - 5-minute getting started guide
3. **quick_start.ipynb** - Interactive Jupyter notebook
4. **config.ini** - Configuration template

---

## ğŸ” Monitoring Training

### TensorBoard
```bash
tensorboard --logdir models/technology_model
```
Open http://localhost:6006

### Watch Training Logs
```bash
tail -f models/technology_model/trainer_state.json
```

### Check GPU Usage
```bash
watch -n 1 nvidia-smi
```

---

## ğŸ¨ Streamlit UI Preview

The web interface includes:

1. **Translation Tab**
   - Input text in English
   - Select domain model
   - Adjust temperature and max tokens
   - View translation with metrics

2. **Compare Domains Tab**
   - Test same text across all models
   - Side-by-side comparison
   - Identify best model for ambiguous text

3. **Evaluation Tab**
   - BLEU and chrF score charts
   - Performance comparison
   - Example translations
   - Interactive visualizations

4. **History Tab**
   - Recent translations
   - Usage statistics
   - Quick access to past results

---

## ğŸ”§ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Solution: Reduce batch size to 2
   - Or: Reduce max_seq_length to 256

2. **Slow Training**
   - Check: GPU is being used (`nvidia-smi`)
   - Verify: CUDA installed correctly
   - Consider: Cloud GPU (Colab, AWS, Lambda Labs)

3. **Import Errors**
   - Run: `pip install -r requirements.txt --upgrade`
   - Check: Python version (3.8+)

4. **Data Loading Fails**
   - Verify: CSV files in `../../Data/english-arabic/`
   - Check: File names match expected names
   - Ensure: CSV has `english` and `arabic` columns

---

## ğŸ¯ Next Steps

After setup, you should:

1. âœ… **Prepare data** - Run `prepare_data.py`
2. âœ… **Train models** - Start with one domain to test
3. âœ… **Evaluate** - Check BLEU/chrF scores
4. âœ… **Test UI** - Try the Streamlit interface
5. âœ… **Fine-tune** - Adjust hyperparameters if needed
6. âœ… **Deploy** - Use for production translations

---

## ğŸ“¦ Dependencies Installed

Core libraries:
- `torch` - PyTorch for deep learning
- `transformers` - Hugging Face models
- `peft` - Parameter-efficient fine-tuning
- `bitsandbytes` - Quantization
- `datasets` - Data loading
- `accelerate` - Multi-GPU training

Evaluation:
- `sacrebleu` - BLEU metrics
- `bert-score` - Semantic similarity

UI and Visualization:
- `streamlit` - Web interface
- `plotly` - Interactive charts
- `matplotlib` - Static plots
- `pandas` - Data manipulation

---

## ğŸŒŸ Project Highlights

âœ¨ **Domain-Specific**: Each domain trained independently
âœ¨ **Memory-Efficient**: Uses QLoRA for 4-bit training
âœ¨ **User-Friendly**: Streamlit UI for easy testing
âœ¨ **Production-Ready**: Complete pipeline from data to deployment
âœ¨ **Well-Documented**: Comprehensive README and guides
âœ¨ **Flexible**: Easily customizable hyperparameters

---

## ğŸ“ Getting Help

1. **Read the docs**: Check `README.md` for detailed info
2. **Quick start**: Follow `QUICKSTART.md` for basics
3. **Test notebook**: Try `quick_start.ipynb` interactively
4. **Check logs**: Review training logs for errors
5. **GPU monitoring**: Use `nvidia-smi` to check resources

---

## ğŸ‰ You're All Set!

Everything is ready for you to start fine-tuning QWEN for domain-specific English-Arabic translation!

**Start with:**
```bash
cd /home/aya/Desktop/ENSIA\ 4Y/S1/NLP/Project/Transformers-2/LLMs/QWEN
python prepare_data.py
```

Then follow the prompts and enjoy! ğŸš€

---

*Created with â¤ï¸ for domain-specific translation*
