# Configuration file for QWEN fine-tuning
# Edit these values to customize your training

[model]
# Base model options: Qwen/Qwen2-1.5B-Instruct, Qwen/Qwen2-7B-Instruct
base_model = Qwen/Qwen2-1.5B-Instruct
use_4bit = true
use_lora = true

[data]
data_dir = data/prepared
max_seq_length = 512
test_size = 0.1
val_size = 0.1

[lora]
lora_r = 16
lora_alpha = 32
lora_dropout = 0.05

[training]
num_train_epochs = 3
per_device_train_batch_size = 4
per_device_eval_batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 2e-4
warmup_ratio = 0.03
logging_steps = 10
save_steps = 100
eval_steps = 100

[domains]
# Domains to train (comma-separated)
enabled = technology,economic,education
